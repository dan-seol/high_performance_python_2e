{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Asynchronous I/O\n",
    "## Async/Await - Coroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An `async` function, defined with `async def` is called a **coroutine**\n",
    "- implemented like generators - able to pause execution and resume as needed\n",
    "- `await` is similar to `yield` statement\n",
    "    - function execution is paused while other codes are running\n",
    "    - once `await/yield` resolves, the execution is resumed\n",
    "    - event loops are responsible for pausing/resuming\n",
    "- Python 2.7\n",
    "    - future based concurrency can get strange when we tried to use coroutines as actual functions\n",
    "    - generators couldn't return values\n",
    "        - third party libraries handle this\n",
    "- Python 3.4+\n",
    "    - easier coroutine creation\n",
    "    - third party libraries have to deal with this awkward transition\n",
    "        - e.g.) `tornado`'s `gen` module\n",
    "- most fully concurrent code's main entry point is primarily setting up / starting the event loop\n",
    "    - this assumes your entire program is concurrent\n",
    "    - `asyncio.loop`\n",
    "        - `loop.run_until_complete(coro: Coroutine)`\n",
    "        - `loop.run_forever()`\n",
    "        - `asyncio.run(coro)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- will analyze a web cralwer that fetches data from an HTTP server that has latency built\n",
    "    - represents the general response-time latency when dealing with I/O\n",
    "    - can support multiple connections at a time\n",
    "        - true for most services\n",
    "        - but if the service cannot handle multiple connections -- will perform only as fast as the serial case\n",
    "- will first create a serial cralwer, as a naive python solution and then\n",
    "- will build up to a full `aiohttp` solution by iterating through `gevent` and `tornado`\n",
    "- finally will combine async I/O tasks with CPU tasks in order to effectively hide any time spent on I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening on port: 8082\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dan-seol/Projects/high_performance_python_2e/08_concurrency/misc/async_await.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dan-seol/Projects/high_performance_python_2e/08_concurrency/misc/async_await.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m http_server\u001b[39m.\u001b[39mlisten(port)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dan-seol/Projects/high_performance_python_2e/08_concurrency/misc/async_await.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m((\u001b[39m\"\u001b[39m\u001b[39mListening on port: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(port)))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dan-seol/Projects/high_performance_python_2e/08_concurrency/misc/async_await.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m ioloop\u001b[39m.\u001b[39;49mIOLoop\u001b[39m.\u001b[39;49minstance()\u001b[39m.\u001b[39;49mstart()\n",
      "File \u001b[0;32m~/micromamba/envs/cpy311/lib/python3.11/site-packages/tornado/platform/asyncio.py:195\u001b[0m, in \u001b[0;36mBaseAsyncIOLoop.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masyncio_loop\u001b[39m.\u001b[39;49mrun_forever()\n",
      "File \u001b[0;32m~/micromamba/envs/cpy311/lib/python3.11/asyncio/base_events.py:596\u001b[0m, in \u001b[0;36mBaseEventLoop.run_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run until stop() is called.\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[0;32m--> 596\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_running()\n\u001b[1;32m    597\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_coroutine_origin_tracking(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debug)\n\u001b[1;32m    599\u001b[0m old_agen_hooks \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mget_asyncgen_hooks()\n",
      "File \u001b[0;32m~/micromamba/envs/cpy311/lib/python3.11/asyncio/base_events.py:588\u001b[0m, in \u001b[0;36mBaseEventLoop._check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_running\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_running():\n\u001b[0;32m--> 588\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThis event loop is already running\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    590\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    591\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mCannot run the event loop while another loop is running\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from tornado import gen, httpserver, ioloop, options, web\n",
    "\n",
    "options.define(\"port\", default=8082, help=\"Port to serve on\")\n",
    "\n",
    "\n",
    "class AddMetric(web.RequestHandler):\n",
    "    metric_data = defaultdict(list)\n",
    "\n",
    "    async def get(self):\n",
    "        if self.get_argument(\"flush\", False):\n",
    "            json.dump(self.metric_data, open(\"metric_data.json\", \"w+\"))\n",
    "        else:\n",
    "            name = self.get_argument(\"name\")\n",
    "            try:\n",
    "                delay = int(self.get_argument(\"delay\", 1024))\n",
    "            except ValueError:\n",
    "                raise web.HTTPError(400, reason=\"Invalid value for delay\")\n",
    "\n",
    "            start = time.time()\n",
    "            await gen.sleep(delay / 1000.0)\n",
    "            self.write(\".\")\n",
    "            self.finish()\n",
    "            end = time.time()\n",
    "            self.metric_data[name].append(\n",
    "                {\"start\": start, \"end\": end, \"dt\": end - start}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- takes a list of URL's\n",
    "- fetches them\n",
    "- sums the total length of the content from the pages\n",
    "- will use a custom http server taking two parameters: `name` and `delay`\n",
    "    - `delay`: how long the server should pause before responding, in milliseconds\n",
    "    - `name`: a name needed for logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    \"\"\"\n",
    "    We add random characters to the end of the URL to break any caching\n",
    "    mechanisms in the requests library or the server\n",
    "    \"\"\"\n",
    "    for i in range(num_urls):\n",
    "        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n",
    "\n",
    "\n",
    "def run_experiment(base_url, num_iter=1000):\n",
    "    response_size = 0\n",
    "    for url in generate_urls(base_url, num_iter):\n",
    "        response = requests.get(url)\n",
    "        response_size += len(response.text)\n",
    "    return response_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1000, Time: 101.58819913864136\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "base_url = f\"http://127.0.0.1:8082/add?name=serial&delay={delay}&\"\n",
    "\n",
    "start = time.time()\n",
    "result = run_experiment(base_url, num_iter)\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- serial: there is no interleaving of our requests\n",
    "- each request takes 100 ms, and there are 500 requests: run time expected to be 50 sec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gevent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- is of paradigm of having async functions return futures\n",
    "    - serial-compatible code\n",
    "- monkey-patches the standard I/O functions to be async\n",
    "    - most of the time we can simply use the standard I/O packages and just monkey-patch it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- produces two mechnisms to enable async programming\n",
    "    - it patches the standard library with async I/O\n",
    "    - it has `Greenlets` object that can be used for concurrent eecution\n",
    "- A **greenlet** is a type of coroutine\n",
    "    - can be thought of as a thread\n",
    "    - however, all greenlets run on the same physical thread\n",
    "    - we have an event loop on a single CPU that is able to switch during I/O wait\n",
    "- gevent tries to make event loop handling as transparent as possible via use of `wait` functions\n",
    "- `wait` will start an event loop and run it as long as is needed for all greenlets\n",
    "    - consequently, most gevent code will run serially\n",
    "    - then at some point, many greenlets to do a concurrent task and start the event loop with the `wait`\n",
    "    - while `wait` is executing, all concurrent tasks queued up will run until completion/stopping condition\n",
    "    - the rest of the code will run serially again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- futures are created with `gevent.spawn`\n",
    "    - which takes a function and the arguments\n",
    "    - then launches a greenlet responsible for running that function\n",
    "    - the greenlet can be a future\n",
    "        - since once the function completes, its value will be contained within the greenlet's `value` field\n",
    "- patching of Python standard modules can make it harder to control the subtleties of what\n",
    "- one thing to ensure when doing async I/O is not to open too many files/connections at once\n",
    "    - could overload the remote server or\n",
    "    - slow down our process by having to context-switch too many times\n",
    "- to limit the number of open files, we use a semaphore as a lockign mechanism\n",
    "- then can wait until `gevent.iwait` , which takes a sequence of futures and iterates over or\n",
    "- `gevent.wait`, which would block execution of our program until all requests are done\n",
    "- semaphore will handle grouping the rqeusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1000, Time: 1.1517233848571777\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from contextlib import closing\n",
    "\n",
    "import gevent\n",
    "from gevent import monkey\n",
    "from gevent.lock import Semaphore\n",
    "\n",
    "monkey.patch_socket()\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    for i in range(num_urls):\n",
    "        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n",
    "\n",
    "\n",
    "def download(url, semaphore):\n",
    "    with semaphore:  # <2>\n",
    "        with closing(urllib.request.urlopen(url)) as data:\n",
    "            return data.read()\n",
    "\n",
    "\n",
    "def chunked_requests(urls, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Given an iterable of urls, this function will yield back the contents of the\n",
    "    URLs. The requests will be batched up in \"chunk_size\" batches using a\n",
    "    semaphore\n",
    "    \"\"\"\n",
    "    semaphore = Semaphore(chunk_size)  # <1>\n",
    "    requests = [gevent.spawn(download, u, semaphore) for u in urls]  # <3>\n",
    "    for response in gevent.iwait(requests):\n",
    "        yield response\n",
    "\n",
    "\n",
    "def run_experiment(base_url, num_iter=1000):\n",
    "    urls = generate_urls(base_url, num_iter)\n",
    "    response_futures = chunked_requests(urls, 100)  # <4>\n",
    "    response_size = sum(len(r.value) for r in response_futures)\n",
    "    return response_size\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "base_url = f\"http://127.0.0.1:8082/add?name=gevent&delay={delay}&\"\n",
    "\n",
    "start = time.time()\n",
    "result = run_experiment(base_url, num_iter)\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N.B. `gevent` is used to make our I/O requests async, but no non-I/O oomputations while in I/O wait\n",
    "    - there is still a massive speed-up by launching more requests while waiting for previous ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tornado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another package for async I/O in Python\n",
    "- originally developed by Facebook, primarily for HTTP clients/servers\n",
    "- framework since Python 3.5\n",
    "- originaly used a system of callbacks - switched to coroutines\n",
    "- currently, can either use `async/await` or `tornado.gen` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1000, Time: 1.1086230278015137\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import string\n",
    "\n",
    "from tornado.httpclient import AsyncHTTPClient\n",
    "\n",
    "AsyncHTTPClient.configure(\n",
    "    \"tornado.curl_httpclient.CurlAsyncHTTPClient\", max_clients=100  # <1>\n",
    ")\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    for i in range(num_urls):\n",
    "        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n",
    "\n",
    "\n",
    "async def run_experiment(base_url, num_iter=1000):\n",
    "    http_client = AsyncHTTPClient()\n",
    "    urls = generate_urls(base_url, num_iter)\n",
    "    response_sum = 0\n",
    "    tasks = [http_client.fetch(url) for url in urls]  # <2>\n",
    "    for task in asyncio.as_completed(tasks):  # <3>\n",
    "        response = await task  # <4>\n",
    "        response_sum += len(response.body)\n",
    "    return response_sum\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "run_func = run_experiment(\n",
    "    f\"http://127.0.0.1:8082/add?name=tornado&delay={delay}&\",\n",
    "    num_iter)\n",
    "\n",
    "start = time.time()\n",
    "result = await run_func  # <5>\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- important difference between `tornado` and `gevent` is when the event loop runs\n",
    "    - `gevent` - the event loop is running only while the `iwait`\n",
    "    - `tornado` - the event loop is running the entire time and controls the complete execution flow of the program\n",
    "- `tornado` - ideal for a mostly I/O-bound application where most of it should be async\n",
    "- `gevent` - ideal for mainly CPU - based that often involves heavy I/O\n",
    "    - e.g.) a lot of computations over a dataset and then ust send the results back to the database\n",
    "    - databases usually have http apis -- can even use `grequests`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another difference is the way the internals change the request call graphs\n",
    "- `gevent` - very uniform call graph shape; new requests are issued the second a lost in the semaphore opens up\n",
    "- `tornado` - is of \"stop-and-go\" shape; the internal limiting mechanism is not robust enough to finishing requests; suboptimal\n",
    "- for all libraries that use asyncio to run the event loop, we can actually replace the backend library\n",
    "    - `uvloop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python 3.4+ revamped the old `asyncio` standard library module, being quite low-level\n",
    "- `aiohttp` is the first popular library built upon the new `asyncio` library\n",
    "- provides both HTTP client, server functionality, and uses a similar API to that of `tornado`\n",
    "- is a part of a greater project `aio-libs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
