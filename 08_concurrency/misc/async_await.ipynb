{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Asynchronous I/O\n",
    "## Async/Await - Coroutines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An `async` function, defined with `async def` is called a **coroutine**\n",
    "- implemented like generators - able to pause execution and resume as needed\n",
    "- `await` is similar to `yield` statement\n",
    "    - function execution is paused while other codes are running\n",
    "    - once `await/yield` resolves, the execution is resumed\n",
    "    - event loops are responsible for pausing/resuming\n",
    "- Python 2.7\n",
    "    - future based concurrency can get strange when we tried to use coroutines as actual functions\n",
    "    - generators couldn't return values\n",
    "        - third party libraries handle this\n",
    "- Python 3.4+\n",
    "    - easier coroutine creation\n",
    "    - third party libraries have to deal with this awkward transition\n",
    "        - e.g.) `tornado`'s `gen` module\n",
    "- most fully concurrent code's main entry point is primarily setting up / starting the event loop\n",
    "    - this assumes your entire program is concurrent\n",
    "    - `asyncio.loop`\n",
    "        - `loop.run_until_complete(coro: Coroutine)`\n",
    "        - `loop.run_forever()`\n",
    "        - `asyncio.run(coro)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- will analyze a web cralwer that fetches data from an HTTP server that has latency built\n",
    "    - represents the general response-time latency when dealing with I/O\n",
    "    - can support multiple connections at a time\n",
    "        - true for most services\n",
    "        - but if the service cannot handle multiple connections -- will perform only as fast as the serial case\n",
    "- will first create a serial cralwer, as a naive python solution and then\n",
    "- will build up to a full `aiohttp` solution by iterating through `gevent` and `tornado`\n",
    "- finally will combine async I/O tasks with CPU tasks in order to effectively hide any time spent on I/O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "from tornado import gen, httpserver, ioloop, options, web\n",
    "\n",
    "options.define(\"port\", default=8082, help=\"Port to serve on\")\n",
    "\n",
    "\n",
    "class AddMetric(web.RequestHandler):\n",
    "    metric_data = defaultdict(list)\n",
    "\n",
    "    async def get(self):\n",
    "        if self.get_argument(\"flush\", False):\n",
    "            json.dump(self.metric_data, open(\"metric_data.json\", \"w+\"))\n",
    "        else:\n",
    "            name = self.get_argument(\"name\")\n",
    "            try:\n",
    "                delay = int(self.get_argument(\"delay\", 1024))\n",
    "            except ValueError:\n",
    "                raise web.HTTPError(400, reason=\"Invalid value for delay\")\n",
    "\n",
    "            start = time.time()\n",
    "            await gen.sleep(delay / 1000.0)\n",
    "            self.write(\".\")\n",
    "            self.finish()\n",
    "            end = time.time()\n",
    "            self.metric_data[name].append(\n",
    "                {\"start\": start, \"end\": end, \"dt\": end - start}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- takes a list of URL's\n",
    "- fetches them\n",
    "- sums the total length of the content from the pages\n",
    "- will use a custom http server taking two parameters: `name` and `delay`\n",
    "    - `delay`: how long the server should pause before responding, in milliseconds\n",
    "    - `name`: a name needed for logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    \"\"\"\n",
    "    We add random characters to the end of the URL to break any caching\n",
    "    mechanisms in the requests library or the server\n",
    "    \"\"\"\n",
    "    for i in range(num_urls):\n",
    "        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n",
    "\n",
    "\n",
    "def run_experiment(base_url, num_iter=1000):\n",
    "    response_size = 0\n",
    "    for url in generate_urls(base_url, num_iter):\n",
    "        response = requests.get(url)\n",
    "        response_size += len(response.text)\n",
    "    return response_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1000, Time: 101.58819913864136\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "base_url = f\"http://127.0.0.1:8082/add?name=serial&delay={delay}&\"\n",
    "\n",
    "start = time.time()\n",
    "result = run_experiment(base_url, num_iter)\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- serial: there is no interleaving of our requests\n",
    "- each request takes 100 ms, and there are 500 requests: run time expected to be 50 sec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gevent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- is of paradigm of having async functions return futures\n",
    "    - serial-compatible code\n",
    "- monkey-patches the standard I/O functions to be async\n",
    "    - most of the time we can simply use the standard I/O packages and just monkey-patch it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- produces two mechnisms to enable async programming\n",
    "    - it patches the standard library with async I/O\n",
    "    - it has `Greenlets` object that can be used for concurrent eecution\n",
    "- A **greenlet** is a type of coroutine\n",
    "    - can be thought of as a thread\n",
    "    - however, all greenlets run on the same physical thread\n",
    "    - we have an event loop on a single CPU that is able to switch during I/O wait\n",
    "- gevent tries to make event loop handling as transparent as possible via use of `wait` functions\n",
    "- `wait` will start an event loop and run it as long as is needed for all greenlets\n",
    "    - consequently, most gevent code will run serially\n",
    "    - then at some point, many greenlets to do a concurrent task and start the event loop with the `wait`\n",
    "    - while `wait` is executing, all concurrent tasks queued up will run until completion/stopping condition\n",
    "    - the rest of the code will run serially again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- futures are created with `gevent.spawn`\n",
    "    - which takes a function and the arguments\n",
    "    - then launches a greenlet responsible for running that function\n",
    "    - the greenlet can be a future\n",
    "        - since once the function completes, its value will be contained within the greenlet's `value` field\n",
    "- patching of Python standard modules can make it harder to control the subtleties of what\n",
    "- one thing to ensure when doing async I/O is not to open too many files/connections at once\n",
    "    - could overload the remote server or\n",
    "    - slow down our process by having to context-switch too many times\n",
    "- to limit the number of open files, we use a semaphore as a lockign mechanism\n",
    "- then can wait until `gevent.iwait` , which takes a sequence of futures and iterates over or\n",
    "- `gevent.wait`, which would block execution of our program until all requests are done\n",
    "- semaphore will handle grouping the rqeusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1000, Time: 1.1517233848571777\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from contextlib import closing\n",
    "\n",
    "import gevent\n",
    "from gevent import monkey\n",
    "from gevent.lock import Semaphore\n",
    "\n",
    "monkey.patch_socket()\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    for i in range(num_urls):\n",
    "        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n",
    "\n",
    "\n",
    "def download(url, semaphore):\n",
    "    with semaphore:  # <2>\n",
    "        with closing(urllib.request.urlopen(url)) as data:\n",
    "            return data.read()\n",
    "\n",
    "\n",
    "def chunked_requests(urls, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Given an iterable of urls, this function will yield back the contents of the\n",
    "    URLs. The requests will be batched up in \"chunk_size\" batches using a\n",
    "    semaphore\n",
    "    \"\"\"\n",
    "    semaphore = Semaphore(chunk_size)  # <1>\n",
    "    requests = [gevent.spawn(download, u, semaphore) for u in urls]  # <3>\n",
    "    for response in gevent.iwait(requests):\n",
    "        yield response\n",
    "\n",
    "\n",
    "def run_experiment(base_url, num_iter=1000):\n",
    "    urls = generate_urls(base_url, num_iter)\n",
    "    response_futures = chunked_requests(urls, 100)  # <4>\n",
    "    response_size = sum(len(r.value) for r in response_futures)\n",
    "    return response_size\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "base_url = f\"http://127.0.0.1:8082/add?name=gevent&delay={delay}&\"\n",
    "\n",
    "start = time.time()\n",
    "result = run_experiment(base_url, num_iter)\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- N.B. `gevent` is used to make our I/O requests async, but no non-I/O oomputations while in I/O wait\n",
    "    - there is still a massive speed-up by launching more requests while waiting for previous ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tornado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another package for async I/O in Python\n",
    "- originally developed by Facebook, primarily for HTTP clients/servers\n",
    "- framework since Python 3.5\n",
    "- originaly used a system of callbacks - switched to coroutines\n",
    "- currently, can either use `async/await` or `tornado.gen` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1000, Time: 1.1086230278015137\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import string\n",
    "\n",
    "from tornado.httpclient import AsyncHTTPClient\n",
    "\n",
    "AsyncHTTPClient.configure(\n",
    "    \"tornado.curl_httpclient.CurlAsyncHTTPClient\", max_clients=100  # <1>\n",
    ")\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    for i in range(num_urls):\n",
    "        yield base_url + \"\".join(random.sample(string.ascii_lowercase, 10))\n",
    "\n",
    "\n",
    "async def run_experiment(base_url, num_iter=1000):\n",
    "    http_client = AsyncHTTPClient()\n",
    "    urls = generate_urls(base_url, num_iter)\n",
    "    response_sum = 0\n",
    "    tasks = [http_client.fetch(url) for url in urls]  # <2>\n",
    "    for task in asyncio.as_completed(tasks):  # <3>\n",
    "        response = await task  # <4>\n",
    "        response_sum += len(response.body)\n",
    "    return response_sum\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "run_func = run_experiment(\n",
    "    f\"http://127.0.0.1:8082/add?name=tornado&delay={delay}&\",\n",
    "    num_iter)\n",
    "\n",
    "start = time.time()\n",
    "result = await run_func  # <5>\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- important difference between `tornado` and `gevent` is when the event loop runs\n",
    "    - `gevent` - the event loop is running only while the `iwait`\n",
    "    - `tornado` - the event loop is running the entire time and controls the complete execution flow of the program\n",
    "- `tornado` - ideal for a mostly I/O-bound application where most of it should be async\n",
    "- `gevent` - ideal for mainly CPU - based that often involves heavy I/O\n",
    "    - e.g.) a lot of computations over a dataset and then ust send the results back to the database\n",
    "    - databases usually have http apis -- can even use `grequests`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- another difference is the way the internals change the request call graphs\n",
    "- `gevent` - very uniform call graph shape; new requests are issued the second a lost in the semaphore opens up\n",
    "- `tornado` - is of \"stop-and-go\" shape; the internal limiting mechanism is not robust enough to finishing requests; suboptimal\n",
    "- for all libraries that use asyncio to run the event loop, we can actually replace the backend library\n",
    "    - `uvloop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python 3.4+ revamped the old `asyncio` standard library module, being quite low-level\n",
    "- `aiohttp` is the first popular library built upon the new `asyncio` library\n",
    "- provides both HTTP client, server functionality, and uses a similar API to that of `tornado`\n",
    "- is a part of a greater project `aio-libs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asyncio http scraper\n",
    "import asyncio\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "def generate_urls(base_url, num_urls):\n",
    "    for i in range(num_urls):\n",
    "        yield os.path.join(base_url, \"\".join(random.sample(string.ascii_lowercase, 10)))\n",
    "\n",
    "\n",
    "def chunked_http_client(num_chunks):\n",
    "    \"\"\"\n",
    "    Returns a function that can fetch from a URL,\n",
    "    ensuring that only \"num_chunks\" of simultaneous connections are made.\n",
    "    \"\"\"\n",
    "    # as in the `gevent` example, we must use a semaphore to limit the number of requests\n",
    "    semaphore = asyncio.Semaphore(num_chunks)\n",
    "\n",
    "    # return a new coroutine that will asynchronously download files and respect the semaphore locking\n",
    "    async def http_get(url, client_session):\n",
    "        nonlocal semaphore\n",
    "        async with semaphore:\n",
    "            async with client_session.request(\"GET\", url) as response:\n",
    "                return await response.content.read()\n",
    "    \n",
    "\n",
    "    return http_get\n",
    "\n",
    "\n",
    "async def run_experiment(base_url, num_iter=1000):\n",
    "    urls = generate_urls(base_url, num_iter)\n",
    "    http_client = chunked_http_client(100)\n",
    "    responses_sum = 0\n",
    "\n",
    "    async with aiohttp.ClientSession() as client_session:\n",
    "        # the http_client function returns futures\n",
    "        # to keep track of progress, we save this into an iterable\n",
    "        tasks = map(lambda url: http_client(url, client_session), urls)\n",
    "\n",
    "        # as with `gevent`, we can wait for futures to become ready and iterate over them\n",
    "        for future in asyncio.as_completed(tasks): #4\n",
    "            data = await future\n",
    "            responses_sum += len(data)\n",
    "\n",
    "    return responses_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 97000, Time: 0.24190711975097656\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "delay = 100\n",
    "num_iter = 1000\n",
    "\n",
    "start = time.time()\n",
    "result = await run_experiment(\n",
    "    f\"http://127.0.0.1:8082/add?name=asyncio&delay={delay}\", num_iter\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Result: {result}, Time: {end - start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- different types of `async` calls and `await`\n",
    "    - `async with`\n",
    "        - `async` context manager, used to get access to shared resources in a concurrent-friendly way\n",
    "        - we allow other coroutines to run while waiting to acquire the resources we are requesting\n",
    "        - consequently, sharing things such as open semaphore slots / already open connections to our host can be done more efficiently than we experienced with `tornado`\n",
    "    - `async def`\n",
    "    - `await`\n",
    "- well behaved call graph\n",
    "    - faster than both `gevent` and `tornado` despite each call takes slightly longer\n",
    "    - this is due to a faster resumption of coroutines paused by the semaphore or waiting for the HTTP client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aiohttp vs tornado\n",
    "    - with aiohttp we are very much in control of the event loop and the various subtleties\n",
    "        - e.g.) we manually acquire the client session\n",
    "        - e.g.) we manually read from the connection\n",
    "    - very useful for real-world application\n",
    "        - can easily add time-outs\n",
    "        - can add functions such as post-process trigger\n",
    "        - for a web server, such control allows us to write \"defensive\" code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
